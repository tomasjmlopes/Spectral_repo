{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multi-head Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial building block of transformers is the Attention mechanism. This step computes the attention between each pair of positions in a sequence and consists of multiple \"attention heads\", each mapping to a different feature space to capture different aspects of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Scaled Dot Product Attention\n",
    "Each head of multi-head attention is computes the scaled dot product attention which can be described as:\n",
    "\n",
    "$$ \\text{SoftMax} \\left( \\frac{Q}{T_e}K^T\\right) V, \\quad T_e = \\sqrt{d_k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        d_model -> dimensionality of the embedding space\n",
    "\n",
    "        This models implement dk as d_model // n_heads \n",
    "        and dv = dk.\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        # Initialize dimenions\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, q, k, v, mask = None, e = 1e-12):\n",
    "        # Here the input should be a 4 dimensional tensor of size\n",
    "        # [batch size, number heads, sequence length, embedding Q,K,V]\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        # 1. First step is to compute QK^T so we need to transpose K\n",
    "        k_t = k.transpose(2, 3)\n",
    "        score = torch.matmul(q, k_t) / math.sqrt(d_tensor)\n",
    "\n",
    "        # 2. Now we need to implement optional masking\n",
    "        if mask is not None:\n",
    "            score.masked_fill(mask == 0, -1e-8)\n",
    "\n",
    "        # SoftMax the scores to obtain probabilities\n",
    "        score = self.softmax(score)\n",
    "\n",
    "        # This value must then be multiplied by the values\n",
    "        attention = score @ v\n",
    "\n",
    "        return attention, score # Score is merely for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "\n",
    "        # Since this models implement dk as d_model // n_heads \n",
    "        # the output of the linear is d*n_heads = d_model\n",
    "        self.wQ = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.wK = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.wV = nn.Linear(d_model, d_model, bias = False)\n",
    "        self.wO = nn.Linear(d_model, d_model, bias = False)\n",
    "\n",
    "    def split(self, tensor):\n",
    "        \"\"\" \n",
    "        This function is used to split the Q, K, V \n",
    "        matrices by their respective number of heads\n",
    "\n",
    "        tensor: [batch_size, sequence_length, dk*n_heads]\n",
    "        returns: [batch_size, n_heads, sequence_length, dk]\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, d_model = tensor.size()\n",
    "        dk = d_model // self.n_heads\n",
    "\n",
    "        tensor = tensor.view(batch_size, seq_length, self.n_heads, dk)\n",
    "        tensor = tensor.transpose(1, 2)\n",
    "\n",
    "        return tensor\n",
    "    \n",
    "    def concat(self, tensor):\n",
    "        \"\"\"\n",
    "        Perform the inverse operation of the split function\n",
    "\n",
    "        tensor: [batch_size, n_heads, sequence_length, dk]\n",
    "        returns: [batch_size, sequence_length, dk*n_heads]\n",
    "        \"\"\"\n",
    "        batch_size, n_heads, seq_length, dk = tensor.size()\n",
    "        d_model = dk*n_heads\n",
    "\n",
    "        # Not sure why contiguous is needed. Try to remove\n",
    "        tensor = tensor.transpose(1, 2).contiguous() \n",
    "        tensor = tensor.view(batch_size, seq_length, d_model)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # 1. Dot Product with the weight matrices\n",
    "        q, k, v = self.Wq(q), self.wK(k), self.wV(v)\n",
    "\n",
    "        # 2. Now we need to split the tensor by the number of heads\n",
    "        q, k, v = q.split(q), k.split(k), v.split(v)\n",
    "\n",
    "        # 3. Calculate Attention\n",
    "        out, attn = self.attention(q, k, v, mask = mask)\n",
    "\n",
    "        # 4. Concatenate and pass to a linear layer\n",
    "        out = self.concat(out)\n",
    "        out = self.wO(out)\n",
    "\n",
    "        # 5. Here should be the implementation to visualize the attention map\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feed Forward Layer\n",
    "\n",
    "This is just a straight forward implementation of a multilayer perceptron (MLP) using two layers. This is place right after the multi-head attention block and its input is the output of the attention block added to a residual connection from the original embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, dropout = 0.1):\n",
    "        super(FFN, self).__init__()\n",
    "        self.layer1 = nn.linear(d_model, d_hidden)\n",
    "        self.layer2 = nn.Linear(d_hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Not sure dropout is where it should be.\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.oens(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize last dimension\n",
    "        mean = x.mean(-1, keepdim = True)\n",
    "        var = x.var(-1, unbiased = True, keepdim = True)\n",
    "\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        # This does nothing, it multiplies by one and adds zero\n",
    "        out = self.gamma * out + self.beta\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encoder/Decoders\n",
    "\n",
    "All the layers necessary for the encoder and decoder blocks have been implemented. It is now a matter of assembling these blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Encoder\n",
    "\n",
    "Starting with the encoder, this piece takes as inputs the embedded sequence, passes them through multi-head attention, followed by a feed forward layer. Both these layers have residual connections in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
