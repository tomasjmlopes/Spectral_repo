{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate numbers using the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    " print(\"Using GPU\")\n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root = './data', train = True, download = False, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = 64, drop_last = True) # drop_last -> discard the last incomplete batch if the dataset size is not divisible by the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataloader) # Convert to iterator\n",
    "images, labels = next(iterator) # Pick the next element in the iterator. In this case it is the first element\n",
    "\n",
    "print(images.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "\n",
    "def show_image_grid(images: torch.Tensor, ncol: int):\n",
    "    image_grid = make_grid(images, ncol)     # Make images into a grid\n",
    "    image_grid = image_grid.permute(1, 2, 0) # Move channel to the last\n",
    "    image_grid = image_grid.cpu().numpy()    # Convert into Numpy\n",
    "    subplots(figsize = (8, 8))\n",
    "    imshow(image_grid)\n",
    "    xticks([])\n",
    "    yticks([])\n",
    "\n",
    "show_image_grid(images, ncol = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decide the size of the random value vector. More random values may potentially add variations to generated images. However, it would take more time and even more difficulty to train the generator as there will be much more network parameters to adjust during the optimization process. Also, MNIST images are not too complex, and we probably do not need too many random values in input vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "generator = torch.nn.Sequential(nn.Linear(100, 128, device = dev),\n",
    "                          nn.LeakyReLU(0.01),\n",
    "                          nn.Linear(128, 784, device = dev),\n",
    "                          nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Converts 100 random values into 128 numeric values via the fully-connected linear layer.\n",
    "* Applies non-linearity via leaky ReLU\n",
    "* Converts 128 numeric values into 784 numeric values (784 = 28x28, which is the image size).\n",
    "* Applies the sigmoid to squeeze output values into the 0 to 1 range (the value range in greyscale images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images():\n",
    "    # Random value inputs (batch size 64)\n",
    "    z = torch.randn(64, 100, device = dev)\n",
    "    # Generator network output\n",
    "    output = generator(z)\n",
    "    # Reshape the output into 64 images\n",
    "    generated_images = output.reshape(64, 1, 28, 28)\n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = generate_images()\n",
    "show_image_grid(generated_images, ncol = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential(nn.Linear(784, 128, device = dev),\n",
    "                              nn.LeakyReLU(0.01),\n",
    "                              nn.Linear(128, 1, device = dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Converts a flattened input image into 128 values via the fully-connected linear layer.\n",
    "* It applies non-linearity via leaky ReLU.\n",
    "* Converts 128 values into one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = discriminator(generated_images.reshape(-1, 784))\n",
    "\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We havenâ€™t trained the discriminator, so it has no idea whether an image is real or fake and outputs meaningless values. Even if we use real MNIST images, the discriminator behaves similarly. So the discriminator can not distinguish between real and fake images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train the discriminator as a classifier (supervised learning), which needs labels. We treat MNIST images as real images and generated images as fake images. In other words, when the discriminator classifies an MNIST image, the label is real (1), and when the discriminator classifies a generated image, the label is fake (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_targets = torch.ones(64, 1, device = dev)\n",
    "fake_targets = torch.zeros(64, 1, device = dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed a batch of MNIST images into the discriminator, we use real_targets; when we feed a batch of generated images into the discriminator, we use fake_targets. Each time, we calculate the cross-entropy loss for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def calculate_loss(images: torch.Tensor, targets: torch.Tensor):\n",
    "    prediction = discriminator(images.reshape(-1, 784))\n",
    "    loss = F.binary_cross_entropy_with_logits(prediction, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr = 1.0e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the discriminator predicts whether an input image is real or fake. We calculate the loss value and apply back-propagation so that the optimizer can adjust network parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On training mode\n",
    "discriminator.train()\n",
    "\n",
    "# On eval mode\n",
    "generator.eval()\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(100)):\n",
    "    for images, labels in dataloader:\n",
    "        # Loss with MNIST image inputs and real_targets as labels\n",
    "        d_loss = calculate_loss(images.to(device), real_targets.to(device))\n",
    "  \n",
    "        # Loss with generated image inputs and fake_targets as labels\n",
    "        generated_images = generate_images()\n",
    "        d_loss += calculate_loss(generated_images, fake_targets)\n",
    "\n",
    "        # Optimizer updates the discriminator parameters\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a usual supervised training for a classification model. The discriminator will learn to distinguish between real (MNIST) images and fake (generated) images. However, the generator network learns nothing in the above training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Generative Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.pyplot import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "device = torch.device(dev)\n",
    "\n",
    "# Configuration\n",
    "epochs      = 100\n",
    "batch_size  = 64\n",
    "sample_size = 100    # Number of random values to sample\n",
    "g_lr        = 1.0e-4 # Generator's learning rate\n",
    "d_lr        = 1.0e-4 # Discriminator's learning rate\n",
    "\n",
    "# DataLoader for MNIST\n",
    "transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, drop_last = True)\n",
    "\n",
    "# Generator Network\n",
    "class Generator(nn.Sequential):\n",
    "    def __init__(self, sample_size: int):\n",
    "        super().__init__(\n",
    "            nn.Linear(sample_size, 128, device = dev),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 784, device = dev),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        # Random value vector size\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def forward(self, batch_size: int):\n",
    "        # Generate randon values\n",
    "        z = torch.randn(batch_size, self.sample_size, device = dev)\n",
    "\n",
    "        # Generator output\n",
    "        output = super().forward(z)\n",
    "\n",
    "        # Convert the output into a greyscale image (1x28x28)\n",
    "        generated_images = output.reshape(batch_size, 1, 28, 28)\n",
    "        return generated_images\n",
    "\n",
    "\n",
    "# Discriminator Network\n",
    "class Discriminator(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Linear(784, 128, device = dev),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1, device = dev))\n",
    "\n",
    "    def forward(self, images: torch.Tensor, targets: torch.Tensor):\n",
    "        prediction = super().forward(images.reshape(-1, 784))\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, targets)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# To save images in grid layout\n",
    "def save_image_grid(epoch: int, images: torch.Tensor, ncol: int):\n",
    "    image_grid = make_grid(images, ncol)     # Images in a grid\n",
    "    image_grid = image_grid.permute(1, 2, 0) # Move channel last\n",
    "    image_grid = image_grid.cpu().numpy()    # To Numpy\n",
    "\n",
    "    subplots()\n",
    "    imshow(image_grid)\n",
    "    xticks([])\n",
    "    yticks([])\n",
    "    # savefig(f'generated_{epoch:03d}.jpg')\n",
    "    close()\n",
    "\n",
    "\n",
    "# Real and fake labels\n",
    "real_targets = torch.ones(batch_size, 1, device = dev)\n",
    "fake_targets = torch.zeros(batch_size, 1, device = dev)\n",
    "\n",
    "\n",
    "# Generator and Discriminator networks\n",
    "generator = Generator(sample_size)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr = d_lr)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr = g_lr)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        #===============================\n",
    "        # Discriminator Network Training\n",
    "        #===============================\n",
    "\n",
    "        # Loss with MNIST image inputs and real_targets as labels\n",
    "        discriminator.train()\n",
    "        d_loss = discriminator(images.to(device), real_targets.to(device))\n",
    "\n",
    "        # Generate images in eval mode\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_images = generator(batch_size)\n",
    "\n",
    "        # Loss with generated image inputs and fake_targets as labels\n",
    "        d_loss += discriminator(generated_images, fake_targets)\n",
    "\n",
    "        # Optimizer updates the discriminator parameters\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        #===============================\n",
    "        # Generator Network Training\n",
    "        #===============================\n",
    "\n",
    "        # Generate images in train mode\n",
    "        generator.train()\n",
    "        generated_images = generator(batch_size)\n",
    "\n",
    "        # Loss with generated image inputs and real_targets as labels\n",
    "        discriminator.eval() # eval but we still need gradients\n",
    "        g_loss = discriminator(generated_images, real_targets)\n",
    "\n",
    "        # Optimizer updates the generator parameters\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Keep losses for logging\n",
    "        d_losses.append(d_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "    # Print average losses\n",
    "    print(epoch, np.mean(d_losses), np.mean(g_losses))\n",
    "\n",
    "    # Save images\n",
    "    # save_image_grid(epoch, generator(batch_size), ncol = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
